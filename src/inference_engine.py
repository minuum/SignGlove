#!/usr/bin/env python3
"""
SignGlove ì¶”ë¡  ì—”ì§„
- ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ ì¶”ë¡ 
- ë‹¤ì–‘í•œ ëª¨ë¸ íƒ€ì… ì§€ì› (BiGRU, CNN, MLP)
"""

import sys
import os
import time
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import json

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    model_type: str  # 'bigru', 'cnn', 'mlp'
    input_size: int = 8
    hidden_size: int = 64
    num_layers: int = 2
    num_classes: int = 24
    dropout: float = 0.2
    model_path: Optional[str] = None

@dataclass
class InferenceResult:
    """ì¶”ë¡  ê²°ê³¼"""
    predicted_class: str
    confidence: float
    probabilities: List[float]
    processing_time: float
    model_type: str
    timestamp: float

class BiGRUModel(nn.Module):
    """BiGRU ëª¨ë¸ ì •ì˜"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        self.gru = nn.GRU(
            input_size=config.input_size,
            hidden_size=config.hidden_size,
            num_layers=config.num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=config.dropout if config.num_layers > 1 else 0
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(config.hidden_size * 2, 128),  # bidirectionalì´ë¯€ë¡œ *2
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(128, config.num_classes)
        )
    
    def forward(self, x):
        # x: (batch_size, sequence_length, input_size)
        gru_out, _ = self.gru(x)
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ ì‚¬ìš©
        last_output = gru_out[:, -1, :]
        logits = self.classifier(last_output)
        return logits

class CNN1DModel(nn.Module):
    """1D CNN ëª¨ë¸ ì •ì˜"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        self.conv_layers = nn.Sequential(
            nn.Conv1d(config.input_size, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(128, config.num_classes)
        )
    
    def forward(self, x):
        # x: (batch_size, sequence_length, input_size)
        x = x.transpose(1, 2)  # (batch_size, input_size, sequence_length)
        conv_out = self.conv_layers(x)
        conv_out = conv_out.squeeze(-1)  # (batch_size, 256)
        logits = self.classifier(conv_out)
        return logits

class MLPModel(nn.Module):
    """MLP ëª¨ë¸ ì •ì˜"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³ ë ¤í•œ ì…ë ¥ í¬ê¸°
        input_features = config.input_size * 80  # 80 time steps
        
        self.classifier = nn.Sequential(
            nn.Linear(input_features, 512),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(128, config.num_classes)
        )
    
    def forward(self, x):
        # x: (batch_size, sequence_length, input_size)
        batch_size = x.size(0)
        x = x.view(batch_size, -1)  # Flatten
        logits = self.classifier(x)
        return logits

class SignGloveInferenceEngine:
    """SignGlove ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.class_names = self._load_class_names()
        
        print(f"ğŸ¤– ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ëª¨ë¸ ìƒì„±
        self._create_model()
        
        # ëª¨ë¸ ë¡œë“œ
        if config.model_path and Path(config.model_path).exists():
            self.load_model(config.model_path)
        else:
            print("âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Mock ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            self.model_loaded = False
        
        print("âœ… ì¶”ë¡  ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ!")
    
    def _load_class_names(self) -> List[str]:
        """í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ"""
        # í•œêµ­ì–´ ìˆ˜ì–´ í´ë˜ìŠ¤ ì •ì˜
        consonants = ["ã„±", "ã„´", "ã„·", "ã„¹", "ã…", "ã…‚", "ã……", "ã…‡", "ã…ˆ", "ã…Š", "ã…‹", "ã…Œ", "ã…", "ã…"]
        vowels = ["ã…", "ã…‘", "ã…“", "ã…•", "ã…—", "ã…›", "ã…œ", "ã… ", "ã…¡", "ã…£"]
        return consonants + vowels
    
    def _create_model(self):
        """ëª¨ë¸ ìƒì„±"""
        if self.config.model_type.lower() == 'bigru':
            self.model = BiGRUModel(self.config)
        elif self.config.model_type.lower() == 'cnn':
            self.model = CNN1DModel(self.config)
        elif self.config.model_type.lower() == 'mlp':
            self.model = MLPModel(self.config)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.config.model_type}")
        
        self.model.to(self.device)
        self.model.eval()
    
    def load_model(self, model_path: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            
            self.model_loaded = True
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            return False
    
    def preprocess_data(self, sensor_data: np.ndarray) -> torch.Tensor:
        """ì„¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        normalized_data = sensor_data.astype(np.float32)
        
        # ê° ì„¼ì„œë³„ ì •ê·œí™”
        # í”Œë ‰ìŠ¤ ì„¼ì„œ (0-1023 -> 0-1)
        normalized_data[:, :5] = normalized_data[:, :5] / 1023.0
        
        # IMU ì„¼ì„œ (ê°ë„ ì •ê·œí™”)
        normalized_data[:, 5:8] = (normalized_data[:, 5:8] + 180) / 360.0
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
        target_length = 80
        if len(normalized_data) < target_length:
            # íŒ¨ë”©
            pad_length = target_length - len(normalized_data)
            padding = np.zeros((pad_length, normalized_data.shape[1]))
            normalized_data = np.vstack([normalized_data, padding])
        elif len(normalized_data) > target_length:
            # ìë¥´ê¸° (ë§ˆì§€ë§‰ 80ê°œ ì‚¬ìš©)
            normalized_data = normalized_data[-target_length:]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        tensor_data = torch.from_numpy(normalized_data).unsqueeze(0)  # (1, 80, 8)
        
        return tensor_data.to(self.device)
    
    def predict(self, sensor_data: np.ndarray) -> InferenceResult:
        """ì¶”ë¡  ì‹¤í–‰"""
        start_time = time.time()
        
        if not self.model_loaded:
            # Mock ì¶”ë¡ 
            return self._mock_predict(sensor_data, start_time)
        
        try:
            # ë°ì´í„° ì „ì²˜ë¦¬
            input_tensor = self.preprocess_data(sensor_data)
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                logits = self.model(input_tensor)
                probabilities = torch.softmax(logits, dim=1)
                confidence, predicted_idx = torch.max(probabilities, 1)
                
                predicted_class = self.class_names[predicted_idx.item()]
                confidence_score = confidence.item()
                prob_list = probabilities.squeeze().cpu().numpy().tolist()
            
            processing_time = time.time() - start_time
            
            return InferenceResult(
                predicted_class=predicted_class,
                confidence=confidence_score,
                probabilities=prob_list,
                processing_time=processing_time,
                model_type=self.config.model_type,
                timestamp=time.time()
            )
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return self._mock_predict(sensor_data, start_time)
    
    def _mock_predict(self, sensor_data: np.ndarray, start_time: float) -> InferenceResult:
        """Mock ì¶”ë¡  (ëª¨ë¸ì´ ì—†ì„ ë•Œ)"""
        # ëœë¤ ì˜ˆì¸¡
        class_idx = np.random.randint(0, len(self.class_names))
        predicted_class = self.class_names[class_idx]
        confidence = np.random.uniform(0.6, 0.95)
        
        # í™•ë¥  ë¶„í¬ ìƒì„±
        probabilities = np.random.dirichlet(np.ones(len(self.class_names)))
        probabilities[class_idx] = confidence
        
        processing_time = time.time() - start_time
        
        return InferenceResult(
            predicted_class=predicted_class,
            confidence=confidence,
            probabilities=probabilities.tolist(),
            processing_time=processing_time,
            model_type=f"{self.config.model_type}_mock",
            timestamp=time.time()
        )
    
    def batch_predict(self, sensor_data_list: List[np.ndarray]) -> List[InferenceResult]:
        """ë°°ì¹˜ ì¶”ë¡ """
        results = []
        for sensor_data in sensor_data_list:
            result = self.predict(sensor_data)
            results.append(result)
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'model_type': self.config.model_type,
            'input_size': self.config.input_size,
            'hidden_size': self.config.hidden_size,
            'num_layers': self.config.num_layers,
            'num_classes': self.config.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_loaded': self.model_loaded,
            'device': str(self.device),
            'class_names': self.class_names
        }

def create_model_config(model_type: str, **kwargs) -> ModelConfig:
    """ëª¨ë¸ ì„¤ì • ìƒì„±"""
    default_configs = {
        'bigru': {
            'input_size': 8,
            'hidden_size': 64,
            'num_layers': 2,
            'num_classes': 24,
            'dropout': 0.2
        },
        'cnn': {
            'input_size': 8,
            'hidden_size': 64,
            'num_layers': 3,
            'num_classes': 24,
            'dropout': 0.2
        },
        'mlp': {
            'input_size': 8,
            'hidden_size': 128,
            'num_layers': 4,
            'num_classes': 24,
            'dropout': 0.3
        }
    }
    
    config_dict = default_configs.get(model_type.lower(), default_configs['bigru'])
    config_dict.update(kwargs)
    
    return ModelConfig(
        model_type=model_type,
        **config_dict
    )

def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("SignGlove ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì„¤ì •
    config = create_model_config('bigru')
    
    # ì¶”ë¡  ì—”ì§„ ìƒì„±
    engine = SignGloveInferenceEngine(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = np.random.randn(80, 8)  # 80 time steps, 8 features
    
    # ì¶”ë¡  ì‹¤í–‰
    result = engine.predict(test_data)
    
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {result.predicted_class}")
    print(f"ì‹ ë¢°ë„: {result.confidence:.3f}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = engine.get_model_info()
    print(f"\nëª¨ë¸ ì •ë³´: {model_info}")

if __name__ == "__main__":
    main()
