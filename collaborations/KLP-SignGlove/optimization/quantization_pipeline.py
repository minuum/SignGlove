"""
KLP-SignGlove ëª¨ë¸ ì–‘ìí™” íŒŒì´í”„ë¼ì¸
ì œê³µëœ ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ ê¸°ë°˜ êµ¬í˜„

Phase 1: ëª¨ë¸ ì¤€ë¹„ ë° ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
Phase 2: ì–‘ìí™” ì‹¤í–‰ (PTQ)
Phase 3: ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë° ë¹„êµ
Phase 4: ë°°í¬
"""

import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.utils.data import DataLoader, Subset
import torch.onnx
import time
import os
import json
import numpy as np
from collections import OrderedDict
from typing import Dict, Tuple, List, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.deep_learning import DeepLearningPipeline
from training.dataset import KSLCsvDataset
from training.train_deep_learning import DeepLearningTrainer

class ModelQuantizationPipeline:
    """ëª¨ë¸ ì–‘ìí™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                 model_path: str = 'best_dl_model.pth',
                 csv_dir: str = 'integrations/SignGlove_HW',
                 output_dir: str = 'optimization/quantized_models'):
        """
        Args:
            model_path: FP32 ëª¨ë¸ ê²½ë¡œ
            csv_dir: ë³´ì •ìš© ë°ì´í„°ì…‹ ê²½ë¡œ
            output_dir: ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.model_path = model_path
        self.csv_dir = csv_dir
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¥ì¹˜ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {self.device}")
        
        # ëª¨ë¸ ì„¤ì •
        self.model_config = {
            'input_features': 8,
            'sequence_length': 20,
            'num_classes': 5,
            'hidden_dim': 128,
            'num_layers': 2,
            'dropout': 0.3
        }
        
        # ì–‘ìí™” ë°±ì—”ë“œ ì„¤ì • (M1 Mac í˜¸í™˜)
        try:
            torch.backends.quantized.engine = 'fbgemm'  # x86 CPU ìµœì í™”
        except RuntimeError:
            torch.backends.quantized.engine = 'qnnpack'  # ARM/Mobile ìµœì í™”
        
        print(f"ğŸ”§ ì–‘ìí™” ë°±ì—”ë“œ: {torch.backends.quantized.engine}")
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = {
            'fp32': {},
            'int8_ptq': {},
            'int8_qat': {}
        }
        
    def phase1_model_preparation(self) -> Tuple[nn.Module, Dict]:
        """
        Phase 1: ëª¨ë¸ ì¤€ë¹„ ë° ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
        """
        print("\n" + "="*60)
        print("ğŸ“€ Phase 1: ëª¨ë¸ ì¤€ë¹„ ë° ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •")
        print("="*60)
        
        # 1. FP32 ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ FP32 ëª¨ë¸ ë¡œë“œ ì¤‘...")
        fp32_model = DeepLearningPipeline(**self.model_config)
        
        if os.path.exists(self.model_path):
            state_dict = torch.load(self.model_path, map_location=self.device)
            fp32_model.load_state_dict(state_dict)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_path}")
        else:
            print(f"âš ï¸  ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ˆê¸°í™” ëª¨ë¸ ì‚¬ìš©: {self.model_path}")
        
        fp32_model.to(self.device)
        fp32_model.eval()
        
        # 2. ë°ì´í„° ì¤€ë¹„
        print("ğŸ“Š ë³´ì • ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        dataset = KSLCsvDataset(
            self.csv_dir,
            window_size=20,
            stride=10
        )
        
        # ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í•  (8:1:1)
        total_size = len(dataset)
        train_size = int(0.8 * total_size)
        val_size = int(0.1 * total_size)
        test_size = total_size - train_size - val_size
        
        # ë³´ì •ìš© ë°ì´í„° (validation set ì‚¬ìš©)
        calibration_indices = list(range(train_size, train_size + val_size))
        calibration_dataset = Subset(dataset, calibration_indices)
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°
        test_indices = list(range(train_size + val_size, total_size))
        test_dataset = Subset(dataset, test_indices)
        
        self.calibration_loader = DataLoader(
            calibration_dataset, batch_size=16, shuffle=False
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=16, shuffle=False
        )
        
        print(f"ğŸ“ˆ ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"  ì „ì²´: {total_size}ê°œ")
        print(f"  ë³´ì •ìš©: {len(calibration_dataset)}ê°œ")
        print(f"  í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")
        
        # 3. ê¸°ì¤€ ì„±ëŠ¥ í‰ê°€
        print("ğŸ¯ FP32 ëª¨ë¸ ê¸°ì¤€ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        fp32_metrics = self._evaluate_model(fp32_model, "FP32")
        
        self.results['fp32'] = fp32_metrics
        
        print(f"\nğŸ“Š FP32 ê¸°ì¤€ ì„±ëŠ¥:")
        print(f"  ì •í™•ë„: {fp32_metrics['accuracy']:.2f}%")
        print(f"  ëª¨ë¸ í¬ê¸°: {fp32_metrics['model_size_mb']:.2f} MB")
        print(f"  ì¶”ë¡  ì†ë„: {fp32_metrics['inference_time_ms']:.2f} ms")
        print(f"  FPS: {fp32_metrics['fps']:.1f}")
        
        return fp32_model, fp32_metrics
    
    def phase2_post_training_quantization(self, fp32_model: nn.Module) -> nn.Module:
        """
        Phase 2: Post-Training Quantization (PTQ) ì‹¤í–‰
        """
        print("\n" + "="*60)
        print("ğŸ”¬ Phase 2: Post-Training Quantization (PTQ)")
        print("="*60)
        
        # 1. ì–‘ìí™” ì„¤ì • ì ìš©
        print("âš™ï¸  ì–‘ìí™” ì„¤ì • ì ìš© ì¤‘...")
        
        # ëª¨ë¸ ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
        ptq_model = DeepLearningPipeline(**self.model_config)
        ptq_model.load_state_dict(fp32_model.state_dict())
        ptq_model.eval()
        
        # ì–‘ìí™” ì„¤ì • (ë°±ì—”ë“œì— ë§ê²Œ)
        backend = torch.backends.quantized.engine
        qconfig = torch.quantization.get_default_qconfig(backend)
        ptq_model.qconfig = qconfig
        
        # ì–‘ìí™” ì¤€ë¹„
        torch.quantization.prepare(ptq_model, inplace=True)
        
        print(f"âœ… ì–‘ìí™” ì„¤ì • ì™„ë£Œ")
        print(f"  QConfig: {qconfig}")
        
        # 2. ëª¨ë¸ ë³´ì • (Calibration)
        print("ğŸ” ëª¨ë¸ ë³´ì • ì¤‘...")
        print("  ëŒ€í‘œ ë°ì´í„°ì…‹ìœ¼ë¡œ í™œì„±í™” ê°’ í†µê³„ ì •ë³´ ìˆ˜ì§‘...")
        
        calibration_samples = 0
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(self.calibration_loader):
                data = data.to(torch.float32)
                
                # ë³´ì • ì‹¤í–‰
                _ = ptq_model(data)
                calibration_samples += data.size(0)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if batch_idx % 10 == 0:
                    print(f"    ë³´ì • ì§„í–‰: {calibration_samples}ê°œ ìƒ˜í”Œ")
        
        print(f"âœ… ë³´ì • ì™„ë£Œ: ì´ {calibration_samples}ê°œ ìƒ˜í”Œ ì‚¬ìš©")
        
        # 3. INT8 ëª¨ë¸ ë³€í™˜
        print("ğŸ”„ INT8 ëª¨ë¸ ë³€í™˜ ì¤‘...")
        int8_model = torch.quantization.convert(ptq_model, inplace=False)
        
        print("âœ… INT8 ë³€í™˜ ì™„ë£Œ")
        
        # 4. ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥
        quantized_model_path = self.output_dir / 'int8_ptq_model.pth'
        torch.save(int8_model.state_dict(), quantized_model_path)
        print(f"ğŸ’¾ ì–‘ìí™” ëª¨ë¸ ì €ì¥: {quantized_model_path}")
        
        return int8_model
    
    def phase3_evaluation_and_comparison(self, 
                                       fp32_model: nn.Module, 
                                       int8_model: nn.Module) -> Dict:
        """
        Phase 3: ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë° ë¹„êµ
        """
        print("\n" + "="*60)
        print("ğŸ“Š Phase 3: ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë° ë¹„êµ")
        print("="*60)
        
        # 1. INT8 ëª¨ë¸ í‰ê°€
        print("ğŸ¯ INT8 ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        int8_metrics = self._evaluate_model(int8_model, "INT8-PTQ")
        
        self.results['int8_ptq'] = int8_metrics
        
        # 2. ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        print("âš–ï¸  ì„±ëŠ¥ ë¹„êµ ë¶„ì„:")
        
        fp32_acc = self.results['fp32']['accuracy']
        int8_acc = self.results['int8_ptq']['accuracy']
        accuracy_drop = fp32_acc - int8_acc
        
        fp32_size = self.results['fp32']['model_size_mb']
        int8_size = self.results['int8_ptq']['model_size_mb']
        size_reduction = (fp32_size - int8_size) / fp32_size * 100
        
        fp32_time = self.results['fp32']['inference_time_ms']
        int8_time = self.results['int8_ptq']['inference_time_ms']
        speed_improvement = (fp32_time - int8_time) / fp32_time * 100
        
        comparison = {
            'accuracy_drop_percent': accuracy_drop,
            'size_reduction_percent': size_reduction,
            'speed_improvement_percent': speed_improvement,
            'compression_ratio': fp32_size / int8_size,
            'needs_qat': accuracy_drop > 5.0  # 5% ì´ìƒ ì •í™•ë„ í•˜ë½ì‹œ QAT í•„ìš”
        }
        
        print(f"\nğŸ“ˆ ë¹„êµ ê²°ê³¼:")
        print(f"  ì •í™•ë„ ë³€í™”: {accuracy_drop:+.2f}% ({fp32_acc:.2f}% â†’ {int8_acc:.2f}%)")
        print(f"  ëª¨ë¸ í¬ê¸° ê°ì†Œ: {size_reduction:.1f}% ({fp32_size:.2f}MB â†’ {int8_size:.2f}MB)")
        print(f"  ì†ë„ ê°œì„ : {speed_improvement:+.1f}% ({fp32_time:.2f}ms â†’ {int8_time:.2f}ms)")
        print(f"  ì••ì¶• ë¹„ìœ¨: {comparison['compression_ratio']:.1f}x")
        
        if comparison['needs_qat']:
            print(f"âš ï¸  ì •í™•ë„ í•˜ë½ì´ {accuracy_drop:.1f}%ë¡œ í½ë‹ˆë‹¤. QAT ê³ ë ¤ í•„ìš”")
        else:
            print(f"âœ… ì •í™•ë„ í•˜ë½ì´ í—ˆìš© ë²”ìœ„({accuracy_drop:.1f}%) ë‚´ì…ë‹ˆë‹¤.")
        
        self.results['comparison'] = comparison
        
        return comparison
    
    def phase4_deployment_export(self, int8_model: nn.Module):
        """
        Phase 4: ë°°í¬ë¥¼ ìœ„í•œ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
        """
        print("\n" + "="*60)
        print("ğŸš€ Phase 4: ë°°í¬ìš© ëª¨ë¸ ë‚´ë³´ë‚´ê¸°")
        print("="*60)
        
        try:
            # 1. ONNX ë³€í™˜
            print("ğŸ“¦ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(1, 20, 8)  # (batch, sequence, features)
            
            onnx_path = self.output_dir / 'int8_model.onnx'
            
            # ONNX ë‚´ë³´ë‚´ê¸° (ì–‘ìí™”ëœ ëª¨ë¸)
            torch.onnx.export(
                int8_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['sensor_data'],
                output_names=['class_logits', 'features', 'attention_weights'],
                dynamic_axes={
                    'sensor_data': {0: 'batch_size'},
                    'class_logits': {0: 'batch_size'},
                    'features': {0: 'batch_size'},
                    'attention_weights': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {onnx_path}")
            print(f"ğŸ“± ì—£ì§€ ë°°í¬ ì¤€ë¹„:")
            print(f"  - Raspberry Pi: ONNX Runtime ì‚¬ìš©")
            print(f"  - Arduino/MCU: TensorFlow Lite Micro ë³€í™˜ í•„ìš”")
            
        except Exception as e:
            print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ëŒ€ì•ˆ: PyTorch Mobile í˜•ì‹ìœ¼ë¡œ ì €ì¥")
            
            # PyTorch Mobile í˜•ì‹ìœ¼ë¡œ ì €ì¥
            mobile_model = torch.jit.script(int8_model)
            mobile_path = self.output_dir / 'int8_model_mobile.pt'
            mobile_model.save(str(mobile_path))
            print(f"âœ… PyTorch Mobile ëª¨ë¸ ì €ì¥: {mobile_path}")
        
        # 2. ë°°í¬ ì •ë³´ íŒŒì¼ ìƒì„±
        deployment_info = {
            'model_info': {
                'type': 'INT8-PTQ',
                'input_shape': [1, 20, 8],
                'output_classes': ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…'],
                'preprocessing': {
                    'flex_normalization': '0-1024 â†’ 0-1',
                    'orientation_normalization': '-180~180 â†’ -1~1'
                }
            },
            'performance': self.results['int8_ptq'],
            'comparison': self.results.get('comparison', {}),
            'deployment_targets': {
                'raspberry_pi': {
                    'runtime': 'ONNX Runtime',
                    'expected_fps': f"{self.results['int8_ptq']['fps']:.1f}",
                    'memory_usage': f"{self.results['int8_ptq']['model_size_mb']:.1f} MB"
                },
                'mobile': {
                    'runtime': 'PyTorch Mobile',
                    'file': 'int8_model_mobile.pt'
                }
            }
        }
        
        info_path = self.output_dir / 'deployment_info.json'
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(deployment_info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ ë°°í¬ ì •ë³´ ì €ì¥: {info_path}")
    
    def _evaluate_model(self, model: nn.Module, model_name: str) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        model.eval()
        
        correct = 0
        total = 0
        inference_times = []
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data = data.to(torch.float32)
                
                # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                outputs = model(data)
                inference_time = (time.time() - start_time) * 1000  # ms
                
                if isinstance(outputs, dict):
                    logits = outputs['class_logits']
                else:
                    logits = outputs
                
                _, predicted = torch.max(logits.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                
                inference_times.append(inference_time)
        
        accuracy = 100 * correct / total
        avg_inference_time = np.mean(inference_times)
        fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        if hasattr(model, 'state_dict'):
            param_size = 0
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            buffer_size = 0
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
            model_size_mb = (param_size + buffer_size) / 1024 / 1024
        else:
            model_size_mb = 0.0
        
        metrics = {
            'accuracy': accuracy,
            'model_size_mb': model_size_mb,
            'inference_time_ms': avg_inference_time,
            'fps': fps,
            'total_samples': total,
            'correct_predictions': correct
        }
        
        return metrics
    
    def optional_quantization_aware_training(self, fp32_model: nn.Module) -> Optional[nn.Module]:
        """
        Optional: Quantization-Aware Training (QAT)
        ì •í™•ë„ í•˜ë½ì´ í° ê²½ìš°ì—ë§Œ ì‹¤í–‰
        """
        print("\n" + "="*60)
        print("âš™ï¸  Optional: Quantization-Aware Training (QAT)")
        print("="*60)
        
        if not self.results.get('comparison', {}).get('needs_qat', False):
            print("âœ… PTQ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ¬ì›Œ QATë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        print("ğŸ”„ QAT ì ìš© ì¤‘...")
        
        # QAT ëª¨ë¸ ì¤€ë¹„
        qat_model = DeepLearningPipeline(**self.model_config)
        qat_model.load_state_dict(fp32_model.state_dict())
        qat_model.train()
        
        # QAT ì„¤ì • (ë°±ì—”ë“œì— ë§ê²Œ)
        backend = torch.backends.quantized.engine
        qat_model.qconfig = torch.quantization.get_default_qat_qconfig(backend)
        torch.quantization.prepare_qat(qat_model, inplace=True)
        
        # ê°„ë‹¨í•œ íŒŒì¸íŠœë‹ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í•™ìŠµ í•„ìš”)
        optimizer = torch.optim.Adam(qat_model.parameters(), lr=0.0001)
        criterion = nn.CrossEntropyLoss()
        
        print("ğŸ“ QAT íŒŒì¸íŠœë‹ ì¤‘...")
        qat_model.train()
        for epoch in range(5):  # ì ì€ ì—í¬í¬ë¡œ íŒŒì¸íŠœë‹
            for batch_idx, (data, targets) in enumerate(self.calibration_loader):
                data = data.to(torch.float32)
                
                optimizer.zero_grad()
                outputs = qat_model(data)
                loss = criterion(outputs['class_logits'], targets)
                loss.backward()
                optimizer.step()
                
                if batch_idx % 10 == 0:
                    print(f"  ì—í¬í¬ {epoch+1}/5, ë°°ì¹˜ {batch_idx}, ì†ì‹¤: {loss.item():.4f}")
        
        # QAT ëª¨ë¸ì„ INT8ë¡œ ë³€í™˜
        qat_model.eval()
        int8_qat_model = torch.quantization.convert(qat_model, inplace=False)
        
        # QAT ëª¨ë¸ í‰ê°€
        qat_metrics = self._evaluate_model(int8_qat_model, "INT8-QAT")
        self.results['int8_qat'] = qat_metrics
        
        print(f"âœ… QAT ì™„ë£Œ")
        print(f"  QAT ì •í™•ë„: {qat_metrics['accuracy']:.2f}%")
        
        # QAT ëª¨ë¸ ì €ì¥
        qat_model_path = self.output_dir / 'int8_qat_model.pth'
        torch.save(int8_qat_model.state_dict(), qat_model_path)
        print(f"ğŸ’¾ QAT ëª¨ë¸ ì €ì¥: {qat_model_path}")
        
        return int8_qat_model
    
    def generate_summary_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì¢…í•© ë³´ê³ ì„œ")
        print("="*60)
        
        # ê²°ê³¼ ì‹œê°í™”
        self._plot_comparison_results()
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ
        report_path = self.output_dir / 'quantization_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("KLP-SignGlove ëª¨ë¸ ì–‘ìí™” ë³´ê³ ì„œ\n")
            f.write("="*50 + "\n\n")
            
            f.write("ğŸ“Š ì„±ëŠ¥ ë¹„êµ:\n")
            for model_type, metrics in self.results.items():
                if model_type != 'comparison' and metrics:
                    f.write(f"\n{model_type.upper()}:\n")
                    f.write(f"  ì •í™•ë„: {metrics['accuracy']:.2f}%\n")
                    f.write(f"  ëª¨ë¸ í¬ê¸°: {metrics['model_size_mb']:.2f} MB\n")
                    f.write(f"  ì¶”ë¡  ì†ë„: {metrics['inference_time_ms']:.2f} ms\n")
                    f.write(f"  FPS: {metrics['fps']:.1f}\n")
            
            if 'comparison' in self.results:
                comp = self.results['comparison']
                f.write(f"\nğŸ“ˆ ê°œì„  íš¨ê³¼:\n")
                f.write(f"  ëª¨ë¸ í¬ê¸° ê°ì†Œ: {comp['size_reduction_percent']:.1f}%\n")
                f.write(f"  ì†ë„ ê°œì„ : {comp['speed_improvement_percent']:+.1f}%\n")
                f.write(f"  ì••ì¶• ë¹„ìœ¨: {comp['compression_ratio']:.1f}x\n")
                f.write(f"  ì •í™•ë„ ë³€í™”: {comp['accuracy_drop_percent']:+.2f}%\n")
        
        print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        # JSON ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / 'quantization_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ë°ì´í„° ì €ì¥: {results_path}")
    
    def _plot_comparison_results(self):
        """ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
        if len(self.results) < 2:
            return
        
        # ë°ì´í„° ì¤€ë¹„
        models = []
        accuracies = []
        sizes = []
        speeds = []
        
        for model_type, metrics in self.results.items():
            if model_type != 'comparison' and metrics:
                models.append(model_type.upper())
                accuracies.append(metrics['accuracy'])
                sizes.append(metrics['model_size_mb'])
                speeds.append(metrics['fps'])
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # ì •í™•ë„ ë¹„êµ
        axes[0].bar(models, accuracies, color=['blue', 'orange', 'green'][:len(models)])
        axes[0].set_title('ëª¨ë¸ ì •í™•ë„ ë¹„êµ')
        axes[0].set_ylabel('ì •í™•ë„ (%)')
        for i, v in enumerate(accuracies):
            axes[0].text(i, v + 0.5, f'{v:.1f}%', ha='center')
        
        # ëª¨ë¸ í¬ê¸° ë¹„êµ
        axes[1].bar(models, sizes, color=['blue', 'orange', 'green'][:len(models)])
        axes[1].set_title('ëª¨ë¸ í¬ê¸° ë¹„êµ')
        axes[1].set_ylabel('í¬ê¸° (MB)')
        for i, v in enumerate(sizes):
            axes[1].text(i, v + 0.05, f'{v:.2f}MB', ha='center')
        
        # FPS ë¹„êµ
        axes[2].bar(models, speeds, color=['blue', 'orange', 'green'][:len(models)])
        axes[2].set_title('ì¶”ë¡  ì†ë„ ë¹„êµ')
        axes[2].set_ylabel('FPS')
        for i, v in enumerate(speeds):
            axes[2].text(i, v + 5, f'{v:.1f}', ha='center')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'quantization_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_full_pipeline(self):
        """ì „ì²´ ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ KLP-SignGlove ëª¨ë¸ ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("ğŸ“‹ ì›Œí¬í”Œë¡œìš°: Phase 1 â†’ Phase 2 â†’ Phase 3 â†’ Phase 4")
        
        try:
            # Phase 1: ëª¨ë¸ ì¤€ë¹„ ë° ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
            fp32_model, fp32_metrics = self.phase1_model_preparation()
            
            # Phase 2: Post-Training Quantization
            int8_ptq_model = self.phase2_post_training_quantization(fp32_model)
            
            # Phase 3: ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë° ë¹„êµ
            comparison = self.phase3_evaluation_and_comparison(fp32_model, int8_ptq_model)
            
            # Optional: QAT (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            int8_qat_model = self.optional_quantization_aware_training(fp32_model)
            
            # Phase 4: ë°°í¬ìš© ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
            final_model = int8_qat_model if int8_qat_model is not None else int8_ptq_model
            self.phase4_deployment_export(final_model)
            
            # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            self.generate_summary_report()
            
            print("\nğŸ‰ ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
            
            return self.results
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

if __name__ == "__main__":
    # ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = ModelQuantizationPipeline(
        model_path='best_dl_model.pth',
        csv_dir='integrations/SignGlove_HW'
    )
    
    results = pipeline.run_full_pipeline()
    
    if results:
        print("\nâœ… ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        
        # ì£¼ìš” ê²°ê³¼ ìš”ì•½
        if 'comparison' in results:
            comp = results['comparison']
            print(f"ğŸ“ˆ ì£¼ìš” ê°œì„  íš¨ê³¼:")
            print(f"  ğŸ—œï¸  ëª¨ë¸ í¬ê¸°: {comp['size_reduction_percent']:.1f}% ê°ì†Œ")
            print(f"  âš¡ ì¶”ë¡  ì†ë„: {comp['speed_improvement_percent']:+.1f}% ê°œì„ ")
            print(f"  ğŸ¯ ì •í™•ë„ ë³€í™”: {comp['accuracy_drop_percent']:+.2f}%")
    else:
        print("âŒ ì–‘ìí™” íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")
