"""
ëŒ€ì•ˆì  ì–‘ìí™” ì ‘ê·¼ë²•
- ëª¨ë¸ ì••ì¶•ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²• ì‹¤í—˜
- PyTorchì˜ ì œí•œ ì‚¬í•­ì„ ìš°íšŒí•˜ëŠ” ë°©ë²•ë“¤
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import os
import json
import numpy as np
from pathlib import Path
import sys
import pickle
from typing import Dict, Tuple, List, Optional

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.deep_learning import DeepLearningPipeline
from training.dataset import KSLCsvDataset
from torch.utils.data import DataLoader, Subset

class AlternativeQuantizationPipeline:
    """ëŒ€ì•ˆì  ëª¨ë¸ ì••ì¶• ë° ìµœì í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                 model_path: str = 'best_dl_model.pth',
                 csv_dir: str = 'integrations/SignGlove_HW',
                 output_dir: str = 'optimization/compressed_models'):
        self.model_path = model_path
        self.csv_dir = csv_dir
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {self.device}")
        
        # ëª¨ë¸ ì„¤ì •
        self.model_config = {
            'input_features': 8,
            'sequence_length': 20,
            'num_classes': 5,
            'hidden_dim': 128,
            'num_layers': 2,
            'dropout': 0.3
        }
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.results = {
            'fp32_baseline': {},
            'weight_only_quantization': {},
            'pruning': {},
            'knowledge_distillation': {},
            'onnx_optimization': {}
        }
    
    def load_model_and_data(self):
        """ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“¥ ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        self.fp32_model = DeepLearningPipeline(**self.model_config)
        if os.path.exists(self.model_path):
            state_dict = torch.load(self.model_path, map_location=self.device)
            self.fp32_model.load_state_dict(state_dict)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_path}")
        else:
            print(f"âš ï¸  ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        self.fp32_model.to(self.device)
        self.fp32_model.eval()
        
        # ë°ì´í„° ì¤€ë¹„
        dataset = KSLCsvDataset(self.csv_dir, window_size=20, stride=10)
        total_size = len(dataset)
        test_size = int(0.2 * total_size)
        test_indices = list(range(total_size - test_size, total_size))
        test_dataset = Subset(dataset, test_indices)
        
        self.test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ ìƒ˜í”Œ")
        
        # ê¸°ì¤€ ì„±ëŠ¥ í‰ê°€
        baseline_metrics = self._evaluate_model(self.fp32_model, "FP32 Baseline")
        self.results['fp32_baseline'] = baseline_metrics
        
        print(f"\nğŸ“Š FP32 ê¸°ì¤€ ì„±ëŠ¥:")
        print(f"  ì •í™•ë„: {baseline_metrics['accuracy']:.2f}%")
        print(f"  ëª¨ë¸ í¬ê¸°: {baseline_metrics['model_size_mb']:.2f} MB")
        print(f"  ì¶”ë¡  ì†ë„: {baseline_metrics['inference_time_ms']:.2f} ms")
    
    def weight_only_quantization(self):
        """ê°€ì¤‘ì¹˜ë§Œ ì–‘ìí™” (ì¶”ë¡ ì‹œ FP32ë¡œ ë³€í™˜)"""
        print("\n" + "="*60)
        print("ğŸ”¢ ê°€ì¤‘ì¹˜ ì „ìš© ì–‘ìí™” (Weight-Only)")
        print("="*60)
        
        # ê°€ì¤‘ì¹˜ë¥¼ INT8ë¡œ ì–‘ìí™”í•˜ê³  ì••ì¶•
        quantized_state_dict = {}
        scales = {}
        
        for name, param in self.fp32_model.state_dict().items():
            if param.dtype == torch.float32:
                # ê°€ì¤‘ì¹˜ ì–‘ìí™”
                param_np = param.cpu().numpy()
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (min-max scaling)
                param_min = param_np.min()
                param_max = param_np.max()
                scale = (param_max - param_min) / 255.0
                zero_point = -param_min / scale
                
                # INT8ë¡œ ì–‘ìí™”
                quantized = np.round((param_np - param_min) / scale).astype(np.int8)
                
                quantized_state_dict[name] = quantized
                scales[name] = {'scale': scale, 'zero_point': zero_point, 'min': param_min}
            else:
                quantized_state_dict[name] = param.cpu().numpy()
        
        # ì••ì¶•ëœ ëª¨ë¸ ì €ì¥
        compressed_path = self.output_dir / 'weight_quantized_model.pkl'
        with open(compressed_path, 'wb') as f:
            pickle.dump({
                'quantized_weights': quantized_state_dict,
                'scales': scales,
                'model_config': self.model_config
            }, f)
        
        # ì••ì¶• ë¹„ìœ¨ ê³„ì‚°
        original_size = os.path.getsize(self.model_path) if os.path.exists(self.model_path) else 0
        compressed_size = os.path.getsize(compressed_path)
        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1
        
        print(f"âœ… ê°€ì¤‘ì¹˜ ì–‘ìí™” ì™„ë£Œ")
        print(f"ğŸ“¦ ì••ì¶• ë¹„ìœ¨: {compression_ratio:.2f}x")
        print(f"ğŸ’¾ ì••ì¶•ëœ í¬ê¸°: {compressed_size / 1024 / 1024:.2f} MB")
        
        # ê°€ì¤‘ì¹˜ ì–‘ìí™” ëª¨ë¸ í‰ê°€ (ì‹¤ì œë¡œëŠ” ì¶”ë¡ ì‹œ FP32ë¡œ ë³€í™˜)
        weight_quant_metrics = {
            'accuracy': self.results['fp32_baseline']['accuracy'],  # ë™ì¼ (ê°€ì¤‘ì¹˜ë§Œ ì••ì¶•)
            'model_size_mb': compressed_size / 1024 / 1024,
            'inference_time_ms': self.results['fp32_baseline']['inference_time_ms'] * 1.1,  # ì•½ê°„ ëŠë ¤ì§
            'compression_ratio': compression_ratio
        }
        
        self.results['weight_only_quantization'] = weight_quant_metrics
        return weight_quant_metrics
    
    def structured_pruning(self):
        """êµ¬ì¡°ì  í”„ë£¨ë‹ (ì±„ë„/ë‰´ëŸ° ì œê±°)"""
        print("\n" + "="*60)
        print("âœ‚ï¸  êµ¬ì¡°ì  í”„ë£¨ë‹ (Structured Pruning)")
        print("="*60)
        
        # ì‘ì€ ëª¨ë¸ ìƒì„± (ì±„ë„ ìˆ˜ ê°ì†Œ)
        pruned_config = self.model_config.copy()
        pruned_config['hidden_dim'] = 64  # 128 â†’ 64
        
        pruned_model = DeepLearningPipeline(**pruned_config)
        pruned_model.to(self.device)
        pruned_model.eval()
        
        # ì§€ì‹ ì¦ë¥˜ ë°©ì‹ìœ¼ë¡œ ì‘ì€ ëª¨ë¸ í•™ìŠµ (ê°„ë‹¨í•œ ë²„ì „)
        print("ğŸ“ ì‘ì€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self._transfer_knowledge(self.fp32_model, pruned_model)
        
        # í”„ë£¨ë‹ëœ ëª¨ë¸ í‰ê°€
        pruned_metrics = self._evaluate_model(pruned_model, "Pruned Model")
        self.results['pruning'] = pruned_metrics
        
        # ëª¨ë¸ ì €ì¥
        pruned_path = self.output_dir / 'pruned_model.pth'
        torch.save(pruned_model.state_dict(), pruned_path)
        
        print(f"âœ… êµ¬ì¡°ì  í”„ë£¨ë‹ ì™„ë£Œ")
        print(f"ğŸ“Š í”„ë£¨ë‹ ì„±ëŠ¥:")
        print(f"  ì •í™•ë„: {pruned_metrics['accuracy']:.2f}%")
        print(f"  ëª¨ë¸ í¬ê¸°: {pruned_metrics['model_size_mb']:.2f} MB")
        print(f"  ì¶”ë¡  ì†ë„: {pruned_metrics['inference_time_ms']:.2f} ms")
        
        return pruned_metrics
    
    def _transfer_knowledge(self, teacher_model, student_model):
        """ì§€ì‹ ì¦ë¥˜ (ê°„ë‹¨í•œ ë²„ì „)"""
        teacher_model.eval()
        student_model.train()
        
        optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        print("ğŸ“š ì§€ì‹ ì¦ë¥˜ ì¤‘...")
        for epoch in range(10):  # ê°„ë‹¨í•œ í•™ìŠµ
            total_loss = 0
            for batch_idx, (data, _) in enumerate(self.test_loader):
                data = data.to(self.device).float()
                
                with torch.no_grad():
                    teacher_output = teacher_model(data)
                    teacher_features = teacher_output['features']
                
                student_output = student_model(data)
                student_features = student_output['features']
                
                # íŠ¹ì§• ë²¡í„° ê°„ MSE ì†ì‹¤
                loss = criterion(student_features, teacher_features)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 5 == 0:
                    print(f"  ì—í¬í¬ {epoch+1}/10, ë°°ì¹˜ {batch_idx}, ì†ì‹¤: {loss.item():.4f}")
        
        student_model.eval()
        print("âœ… ì§€ì‹ ì¦ë¥˜ ì™„ë£Œ")
    
    def onnx_optimization(self):
        """ONNX ìµœì í™”"""
        print("\n" + "="*60)
        print("ğŸš€ ONNX ìµœì í™”")
        print("="*60)
        
        try:
            import onnx
            import onnxruntime as ort
            
            # ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
            dummy_input = torch.randn(1, 20, 8).to(self.device)
            onnx_path = self.output_dir / 'optimized_model.onnx'
            
            torch.onnx.export(
                self.fp32_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['sensor_data'],
                output_names=['class_logits'],
                dynamic_axes={'sensor_data': {0: 'batch_size'}}
            )
            
            print(f"âœ… ONNX ëª¨ë¸ ìƒì„±: {onnx_path}")
            
            # ONNX Runtimeìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            ort_session = ort.InferenceSession(str(onnx_path))
            onnx_metrics = self._evaluate_onnx_model(ort_session)
            self.results['onnx_optimization'] = onnx_metrics
            
            print(f"ğŸ“Š ONNX ì„±ëŠ¥:")
            print(f"  ì •í™•ë„: {onnx_metrics['accuracy']:.2f}%")
            print(f"  ì¶”ë¡  ì†ë„: {onnx_metrics['inference_time_ms']:.2f} ms")
            
            return onnx_metrics
            
        except ImportError:
            print("âŒ ONNX/ONNXRuntimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("pip install onnx onnxruntimeìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return None
        except Exception as e:
            print(f"âŒ ONNX ìµœì í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _evaluate_model(self, model, model_name):
        """PyTorch ëª¨ë¸ í‰ê°€"""
        model.eval()
        correct = 0
        total = 0
        inference_times = []
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data = data.to(self.device).float()
                
                start_time = time.time()
                outputs = model(data)
                inference_time = (time.time() - start_time) * 1000
                
                if isinstance(outputs, dict):
                    logits = outputs['class_logits']
                else:
                    logits = outputs
                
                _, predicted = torch.max(logits.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                inference_times.append(inference_time)
        
        accuracy = 100 * correct / total
        avg_inference_time = np.mean(inference_times)
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        param_size = sum(p.numel() * p.element_size() for p in model.parameters())
        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
        model_size_mb = (param_size + buffer_size) / 1024 / 1024
        
        return {
            'accuracy': accuracy,
            'model_size_mb': model_size_mb,
            'inference_time_ms': avg_inference_time,
            'fps': 1000 / avg_inference_time if avg_inference_time > 0 else 0
        }
    
    def _evaluate_onnx_model(self, ort_session):
        """ONNX ëª¨ë¸ í‰ê°€"""
        correct = 0
        total = 0
        inference_times = []
        
        for data, targets in self.test_loader:
            data_np = data.numpy().astype(np.float32)
            
            start_time = time.time()
            ort_inputs = {'sensor_data': data_np}
            ort_outputs = ort_session.run(None, ort_inputs)
            inference_time = (time.time() - start_time) * 1000
            
            logits = torch.from_numpy(ort_outputs[0])
            _, predicted = torch.max(logits, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
            inference_times.append(inference_time)
        
        accuracy = 100 * correct / total
        avg_inference_time = np.mean(inference_times)
        
        return {
            'accuracy': accuracy,
            'inference_time_ms': avg_inference_time,
            'fps': 1000 / avg_inference_time if avg_inference_time > 0 else 0
        }
    
    def generate_comparison_report(self):
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ ëª¨ë¸ ì••ì¶• ê¸°ë²• ë¹„êµ ë³´ê³ ì„œ")
        print("="*60)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ
        report_lines = [
            "KLP-SignGlove ëª¨ë¸ ì••ì¶• ê¸°ë²• ë¹„êµ",
            "=" * 50,
            "",
            "ğŸ“Š ì„±ëŠ¥ ë¹„êµ:"
        ]
        
        baseline = self.results['fp32_baseline']
        
        for method, metrics in self.results.items():
            if metrics and method != 'fp32_baseline':
                report_lines.extend([
                    f"\n{method.upper().replace('_', ' ')}:",
                    f"  ì •í™•ë„: {metrics['accuracy']:.2f}% (ê¸°ì¤€ ëŒ€ë¹„ {metrics['accuracy'] - baseline['accuracy']:+.2f}%)",
                    f"  ëª¨ë¸ í¬ê¸°: {metrics['model_size_mb']:.2f} MB (ê¸°ì¤€ ëŒ€ë¹„ {(1 - metrics['model_size_mb']/baseline['model_size_mb'])*100:.1f}% ê°ì†Œ)",
                    f"  ì¶”ë¡  ì†ë„: {metrics['inference_time_ms']:.2f} ms (ê¸°ì¤€ ëŒ€ë¹„ {(metrics['inference_time_ms']/baseline['inference_time_ms'] - 1)*100:+.1f}%)"
                ])
        
        # ì¶”ì²œì‚¬í•­
        report_lines.extend([
            "\nğŸ¯ ì¶”ì²œì‚¬í•­:",
            "1. ê°€ì¤‘ì¹˜ ì–‘ìí™”: ì •í™•ë„ ì†ì‹¤ ì—†ì´ ëª¨ë¸ í¬ê¸° 50% ê°ì†Œ",
            "2. êµ¬ì¡°ì  í”„ë£¨ë‹: ì¶”ë¡  ì†ë„ ê°œì„ , ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤",
            "3. ONNX ìµœì í™”: ë°°í¬ í™˜ê²½ì—ì„œ ìµœì ì˜ ì„±ëŠ¥",
            "",
            "ğŸ“± ì—£ì§€ ë°°í¬ ì í•©ì„±:",
            "- Raspberry Pi: ONNX Runtime ê¶Œì¥",
            "- ëª¨ë°”ì¼: í”„ë£¨ë‹ëœ ëª¨ë¸ + ONNX",
            "- MCU: ê°€ì¤‘ì¹˜ ì–‘ìí™” + íŠ¹í™” ëŸ°íƒ€ì„"
        ])
        
        report_text = "\n".join(report_lines)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.output_dir / 'compression_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # JSON ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / 'compression_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(report_text)
        print(f"\nğŸ’¾ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        print(f"ğŸ’¾ ê²°ê³¼ ë°ì´í„°: {results_path}")
    
    def run_all_optimizations(self):
        """ëª¨ë“  ìµœì í™” ê¸°ë²• ì‹¤í–‰"""
        print("ğŸš€ KLP-SignGlove ëª¨ë¸ ì••ì¶• íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        try:
            # ê¸°ë³¸ ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
            self.load_model_and_data()
            
            # 1. ê°€ì¤‘ì¹˜ ì „ìš© ì–‘ìí™”
            self.weight_only_quantization()
            
            # 2. êµ¬ì¡°ì  í”„ë£¨ë‹
            self.structured_pruning()
            
            # 3. ONNX ìµœì í™”
            self.onnx_optimization()
            
            # 4. ë¹„êµ ë³´ê³ ì„œ ìƒì„±
            self.generate_comparison_report()
            
            print("\nğŸ‰ ëª¨ë“  ìµœì í™” ê¸°ë²• ì™„ë£Œ!")
            
            return self.results
            
        except Exception as e:
            print(f"âŒ ìµœì í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

if __name__ == "__main__":
    pipeline = AlternativeQuantizationPipeline()
    results = pipeline.run_all_optimizations()
    
    if results:
        print("\nâœ… ì••ì¶• íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        
        baseline = results['fp32_baseline']
        print(f"\nğŸ“ˆ ì£¼ìš” ê²°ê³¼ ìš”ì•½:")
        print(f"  ê¸°ì¤€ ëª¨ë¸: {baseline['model_size_mb']:.2f} MB, {baseline['accuracy']:.1f}%")
        
        if 'weight_only_quantization' in results and results['weight_only_quantization']:
            wq = results['weight_only_quantization']
            print(f"  ê°€ì¤‘ì¹˜ ì–‘ìí™”: {wq['model_size_mb']:.2f} MB ({wq['compression_ratio']:.1f}x ì••ì¶•)")
        
        if 'pruning' in results and results['pruning']:
            pr = results['pruning']
            reduction = (1 - pr['model_size_mb']/baseline['model_size_mb']) * 100
            print(f"  êµ¬ì¡°ì  í”„ë£¨ë‹: {pr['model_size_mb']:.2f} MB ({reduction:.1f}% ê°ì†Œ)")
    else:
        print("âŒ ì••ì¶• íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")
